{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "from html.parser import HTMLParser\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import ssl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcameron\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'issasports.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# Get links for all finals\n",
    "url = 'https://issasports.com/results/champs08/evtindex.htm'\n",
    "master_url = 'https://issasports.com/results/champs08/'\n",
    "page = requests.get(url, verify=False)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "links = []\n",
    "a_tags = soup.find_all('a')\n",
    "for tag in a_tags:\n",
    "    if 'Prelims' not in tag.text and 'Semis' not in tag.text and 'Latest' not in tag.text:\n",
    "        links.append(master_url + tag['href'])\n",
    "\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pages based on links collected\n",
    "count = 0\n",
    "for link in links:\n",
    "    try:\n",
    "        page = requests.get(link, verify=False).text\n",
    "        filepath = os.path.join(r'../champs/web_pages/2008', f'{count}.html')\n",
    "        f = open(filepath, 'w')\n",
    "        f.write(page)\n",
    "        f.close()\n",
    "        count += 1\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through directory and rename files\n",
    "directory = '../champs/web_pages/2008'\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        try:\n",
    "            with open(f) as page:\n",
    "                soup = BeautifulSoup(page, 'html.parser')\n",
    "                r = soup.find('pre').text\n",
    "                r = r.splitlines()\n",
    "\n",
    "                # Get event details\n",
    "                event_details = r[5]\n",
    "                event_details = event_details.split()\n",
    "                if event_details[6].strip().lower() != 'class':\n",
    "                    event = event_details[4] + \" \" + event_details[5].strip()+ \" \" + event_details[6].strip()\n",
    "                else:\n",
    "                    event = event_details[4] + \" \" + event_details[5].strip()\n",
    "                cl = event_details[-1] if event_details[-1].isnumeric() else event_details[-2].strip()\n",
    "                gender = event_details[2].strip()\n",
    "                new_name = f'Class_{cl}_{gender}_{event}.html'\n",
    "                new_name = os.path.join(directory, new_name).replace('/', '\\\\')\n",
    "            os.rename(f, new_name)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results data from web pages\n",
    "from parse_data import get_races, get_field_events, get_relays\n",
    "directory = '../champs/web_pages/2019'\n",
    "clms = ['event', 'class', 'gender', 'position', 'name', 'school', 'mark', 'points']\n",
    "df = pd.DataFrame(columns=clms)\n",
    "\n",
    "# Loop through files and get results data\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "\n",
    "    # Get Race results data from web pages\n",
    "    if os.path.isfile(f) and ('Dash' in filename or 'Run' in filename or 'Hurdles' in filename ):\n",
    "        with open(f) as page:\n",
    "            data = get_races(page) # Imp function to parse event data\n",
    "            frame = pd.DataFrame(data, columns=clms)\n",
    "            df = df.append(frame) # Append each event to dataframe\n",
    "    \n",
    "    # Get Field Events results data from web pages\n",
    "    if os.path.isfile(f) and ('Throw' in filename or 'Put' in filename or 'Jump' in filename ):\n",
    "        with open(f) as page:\n",
    "            data = get_field_events(page) # Imp function to parse event data\n",
    "            frame = pd.DataFrame(data, columns=clms)\n",
    "            df = df.append(frame) # Append each event to dataframe\n",
    "\n",
    "     # Get Relay results data from web pages\n",
    "    if os.path.isfile(f) and 'Relay' in filename:\n",
    "        with open(f) as page:\n",
    "            data = get_relays(page) # Imp function to parse event data\n",
    "            frame = pd.DataFrame(data, columns=clms)\n",
    "            df = df.append(frame) # Append each event to dataframe\n",
    "\n",
    "# Create CSV file with results data\n",
    "df.to_csv('../champs/csv_files' + '\\\\' + 'champs_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 87  Heptathlon: #7 Girls 13-19 800 Meter Run\n",
      "                        Women - Team Rankings - 38 Events Scored             \n",
      "- 38 Events\n"
     ]
    }
   ],
   "source": [
    "page = '../champs/web_pages/2022/87.html'\n",
    "with open(page) as pg:\n",
    "\n",
    "    soup = BeautifulSoup(pg, 'html.parser')\n",
    "    r = soup.find('pre').text\n",
    "    r = r.splitlines()\n",
    "    lst = []\n",
    "\n",
    "    # Get event details\n",
    "    for line in r:\n",
    "        if 'Event' in line.strip():\n",
    "            event_details = line\n",
    "            print(event_details)\n",
    "\n",
    "    event_details = event_details.split()\n",
    "    #print(event_details)\n",
    "    if event_details[6].strip().lower() != 'class':\n",
    "        event = event_details[4] + \" \" + event_details[5].strip()+ \" \" + event_details[6].strip()\n",
    "    else:\n",
    "        event = event_details[4] + \" \" + event_details[5].strip()\n",
    "    print(event)\n",
    "    cl = event_details[-1] if event_details[-1].isnumeric() else event_details[-2].strip()\n",
    "    gender = event_details[2].strip()\n",
    "    \n",
    "    # Event Results\n",
    "    for line in r:\n",
    "        my_dict = {}\n",
    "        if len(line) > 0:\n",
    "            clean_line = line.split('  ')\n",
    "            clean_line = list(filter(None, clean_line))\n",
    "            #print(clean_line)\n",
    "            if (clean_line[0][0].isnumeric() and clean_line[0][1] != ')') or (clean_line[0].strip()[0:2].isnumeric() and clean_line[0].strip()[2] != ')'):\n",
    "                if (clean_line[0].strip()[2] != '.' and clean_line[0].strip()[1] != '.' ):\n",
    "                    position = clean_line[0].strip().split(' ',1)[0]\n",
    "                    school = clean_line[0].strip().split(' ',1)[1]\n",
    "\n",
    "                    my_dict['event'] = event\n",
    "                    my_dict['class'] = cl\n",
    "                    my_dict['gender'] = gender\n",
    "                    my_dict['position'] = position\n",
    "                    my_dict['name'] = ''\n",
    "                    my_dict['school'] = school\n",
    "            \n",
    "                    try:\n",
    "                        if clean_line[-1].strip() != '' and clean_line[-1].strip().isnumeric():\n",
    "                            points = clean_line[-1].strip()\n",
    "                        elif clean_line[-1].strip() == '' and clean_line[-2].strip().isnumeric():\n",
    "                            points = clean_line[-2].strip()\n",
    "                        else:\n",
    "                            points = 0\n",
    "                        mark = clean_line[1].strip()\n",
    "                        my_dict['mark'] = mark\n",
    "                        my_dict['points'] = points\n",
    "                    except IndexError:\n",
    "                        points = 0\n",
    "                        mark = clean_line[-2].strip()\n",
    "                        my_dict['mark'] = mark\n",
    "                        my_dict['points'] = points\n",
    "                    lst.append(my_dict)\n",
    "    #print(lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5657643e5048a81eaf5dd159b1be86affd2cfe37dd093a16d000a3fcd4024fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
